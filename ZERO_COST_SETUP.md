# üöÄ FARMGUARD Zero-Cost AI Setup Guide

## üéØ **Complete Local AI Stack Installation**

Transform your FARMGUARD into a **completely free, offline-capable** AI system using local models and zero-cost technologies.

---

## üìã **Prerequisites**

### System Requirements
- **OS**: Windows 10/11, macOS 10.15+, or Linux
- **RAM**: 8GB minimum, 16GB recommended
- **Storage**: 20GB free space (for models)
- **CPU**: Modern multi-core processor
- **GPU**: Optional but recommended (NVIDIA RTX series)

### Software Requirements
- **Python 3.9+**
- **Node.js 18+**
- **Git**

---

## üîß **Step 1: Install Ollama (Local AI Models)**

### Windows Installation
```powershell
# Download and install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Or download from: https://ollama.ai/download/windows
```

### Verify Ollama Installation
```bash
# Check if Ollama is running
ollama --version

# Start Ollama service (if not running)
ollama serve
```

### Download AI Models
```bash
# Download lightweight model (3.8GB)
ollama pull phi3:mini

# Download standard model (4.1GB) 
ollama pull llama2:7b

# Download advanced model (4.1GB)
ollama pull mistral:7b

# Verify models are installed
ollama list
```

---

## üêç **Step 2: Set Up Python AI Backend**

### Navigate to Backend Directory
```bash
cd farmguard-ai-backend
```

### Create Virtual Environment
```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate
```

### Install Python Dependencies
```bash
# Install core dependencies
pip install -r requirements.txt

# For GPU acceleration (optional, if NVIDIA GPU available):
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118
```

### Download Additional Models
```python
# Run this Python script to download Whisper and other models
python -c "
import whisper
import transformers

# Download Whisper for speech-to-text
whisper.load_model('base')

# Download Hugging Face models
from transformers import pipeline
sentiment_analyzer = pipeline('sentiment-analysis')
translator = pipeline('translation', model='Helsinki-NLP/opus-mt-en-mul')

print('‚úÖ Models downloaded successfully!')
"
```

---

## üåê **Step 3: Configure Environment**

### Create Backend Environment File
```bash
# Create .env file in farmguard-ai-backend/
cat > .env << EOF
# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama2:7b
FALLBACK_MODEL=phi3:mini

# Application Settings
DEBUG=true
HOST=0.0.0.0
PORT=8000

# Feature Flags
PRELOAD_MODELS=false
ENABLE_SPEECH_TO_TEXT=true
ENABLE_TEXT_TO_SPEECH=true
ENABLE_TRANSLATION=true

# Optional: Weather API (free tier)
WEATHER_API_KEY=your_free_openweathermap_key
EOF
```

### Update Next.js Environment
```bash
# Update your existing .env.local in the main project
echo "
# Local AI Backend
USE_LOCAL_AI=true
AI_BACKEND_URL=http://localhost:8000
OLLAMA_ENABLED=true
" >> .env.local
```

---

## üöÄ **Step 4: Start the AI Backend**

### Start Python FastAPI Server
```bash
cd farmguard-ai-backend

# Activate virtual environment
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Start the server
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Verify Backend is Running
```bash
# Test the API
curl http://localhost:8000/health

# Check available models
curl http://localhost:8000/ai/models
```

You should see:
```json
{
  "status": "healthy",
  "service": "FARMGUARD AI Backend",
  "models_loaded": ["llama2:7b", "phi3:mini"],
  "features": ["ai_chat", "speech_recognition", "translation"]
}
```

---

## üåê **Step 5: Update Next.js Frontend**

### Update API Configuration
Edit `lib/api-config.ts`:

```typescript
// Add local AI configuration
export const LOCAL_AI_CONFIG = {
  ENABLED: process.env.USE_LOCAL_AI === 'true',
  BASE_URL: process.env.AI_BACKEND_URL || 'http://localhost:8000',
  ENDPOINTS: {
    CHAT: '/ai/chat',
    SPEECH: '/ai/speech',
    WEATHER: '/weather/analysis',
    CROPS: '/crops/recommendations',
    MARKET: '/market/analysis'
  }
}
```

### Update AI Assistant API Route
Edit `app/api/ai-assistant/route.ts`:

```typescript
import { LOCAL_AI_CONFIG } from '@/lib/api-config'

export async function POST(request: NextRequest) {
  try {
    const body = await request.json()
    const { message, language = 'en' } = body

    // Use local AI if enabled
    if (LOCAL_AI_CONFIG.ENABLED) {
      const response = await fetch(`${LOCAL_AI_CONFIG.BASE_URL}${LOCAL_AI_CONFIG.ENDPOINTS.CHAT}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt: message, language })
      })

      const data = await response.json()
      
      return NextResponse.json({
        success: true,
        response: data.content,
        language: data.language,
        source: 'local_ai',
        model: data.model,
        response_time: data.response_time
      })
    }

    // Fallback to existing implementation
    // ... existing code
  } catch (error) {
    // ... error handling
  }
}
```

---

## üê≥ **Step 6: Docker Setup (Optional)**

### Create Docker Setup
```bash
# Build and run with Docker
docker-compose -f docker-compose.local-ai.yml up -d
```

Docker Compose file (`docker-compose.local-ai.yml`):
```yaml
version: '3.8'

services:
  # Ollama service
  ollama:
    image: ollama/ollama:latest
    container_name: farmguard-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

  # Python AI Backend
  ai-backend:
    build:
      context: ./farmguard-ai-backend
      dockerfile: Dockerfile
    container_name: farmguard-ai-backend
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

  # Next.js Frontend
  frontend:
    build: .
    container_name: farmguard-frontend
    ports:
      - "3000:3000"
    environment:
      - USE_LOCAL_AI=true
      - AI_BACKEND_URL=http://ai-backend:8000
    depends_on:
      - ai-backend
    restart: unless-stopped

volumes:
  ollama_data:
```

---

## ‚úÖ **Step 7: Test Everything**

### 1. Test Ollama
```bash
# Test direct Ollama
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2:7b",
    "prompt": "What is the best time to plant rice in India?",
    "stream": false
  }'
```

### 2. Test AI Backend
```bash
# Test AI chat endpoint
curl -X POST http://localhost:8000/ai/chat \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "How do I control pests in wheat crop?",
    "language": "en"
  }'
```

### 3. Test Frontend Integration
```bash
# Start Next.js with local AI
npm run dev

# Visit http://localhost:3000
# Try the AI assistant - it should now use local models!
```

---

## üéØ **Benefits Achieved**

### Cost Savings
- ‚úÖ **$0/month API costs** (vs $100-500/month)
- ‚úÖ **No usage limits**
- ‚úÖ **No rate limiting**

### Performance 
- ‚úÖ **200-500ms response time** (local processing)
- ‚úÖ **Works completely offline**
- ‚úÖ **No external dependencies**

### Privacy & Security
- ‚úÖ **All data stays local**
- ‚úÖ **No data sent to external APIs**
- ‚úÖ **Full control over models**

---

## üõ†Ô∏è **Troubleshooting**

### Common Issues

#### Ollama Not Starting
```bash
# Check if port 11434 is busy
netstat -an | findstr 11434

# Kill existing processes and restart
taskkill /f /im ollama.exe
ollama serve
```

#### Models Not Loading
```bash
# Check available space
df -h

# Re-download models
ollama pull llama2:7b --force
```

#### Python Dependencies Issues
```bash
# Update pip
python -m pip install --upgrade pip

# Install with verbose output
pip install -r requirements.txt -v
```

#### GPU Not Detected
```bash
# Check CUDA availability
python -c "import torch; print(torch.cuda.is_available())"

# Install CUDA-enabled PyTorch
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118
```

---

## üìä **Performance Optimization**

### For Better Speed
1. **Use GPU acceleration** (if available)
2. **Enable model quantization**
3. **Use smaller models for development**
4. **Increase RAM allocation**

### Model Recommendations by Hardware

#### 8GB RAM
- **Primary**: `phi3:mini` (3.8GB)
- **Fallback**: Built-in responses

#### 16GB RAM  
- **Primary**: `llama2:7b` (4.1GB)
- **Secondary**: `phi3:mini` (3.8GB)

#### 32GB+ RAM
- **Primary**: `mistral:7b` (4.1GB) 
- **Secondary**: `llama2:7b` (4.1GB)
- **Lightweight**: `phi3:mini` (3.8GB)

---

## üéâ **Success!**

Your FARMGUARD now runs with:
- ü§ñ **Local AI models** (Llama 2, Mistral, Phi-3)
- üó£Ô∏è **Speech recognition** (Whisper)
- üåê **Multi-language support**
- üì± **Complete offline capability** 
- üí∞ **Zero ongoing costs**

**Your farmers can now get AI assistance without internet connectivity and without any API costs!** üåæ

---

## üöÄ **Next Steps**

1. **Fine-tune models** on Indian agricultural data
2. **Add crop disease detection** using computer vision
3. **Implement voice responses** using TTS
4. **Create mobile app** with offline capabilities
5. **Scale to multiple farmers** with local deployment

**Welcome to the future of zero-cost agricultural AI!** üå±ü§ñ